////
changes: ["Initial Zip/Unzip with Progress project chapter for Zig 0.15.2"]
examples_compile: yes
keywords: ["zip", "progress", "StringHashMap", "crc32", "Timer"]
last_updated: 2025-11-13
last_verified: 2025-11-13
previous_chapter: "53__project-top-k-word-frequency-analyzer"
next_chapter: ""
status: draft
xref_complete: true
open_questions: []
////

= Project: Zip/Unzip with Progress
:chapter-number: 54
:chapter-slug: project-zip-unzip-with-progress
:doctype: book
:embedded:
:experimental:
:icons: font
:partnums:
:pygments-linenums-mode: inline
:pygments-style: manni
:safe-mode-level: 0
:sectanchors:
:sectids:
:sectlinks:
:source-highlighter: pygments
:sourcedir: example$chapters-data/code
:webfonts:
:xrefstyle: short
:zig-version: 0.15.2
:linkcss:
:stylesdir: styles
:stylesheet: zigbook.css

[[overview]]
== Overview

The previous project focused on deterministic text analytics; now we bundle those artifacts and the surrounding diagnostics into a reproducible archive pipeline. xref:53__project-top-k-word-frequency-analyzer.adoc[53] We will write a minimalist ZIP creator that streams files into memory, emits the central directory, then verifies extraction while reporting incremental progress. The program leans on the standard library's ZIP reader, manual header encoding, `StringHashMap` bookkeeping for CRC32 checks, and structured status updates through `std.Progress`. link:https://github.com/ziglang/zig/tree/master/lib/std/zip.zig[zip.zig] link:https://github.com/ziglang/zig/tree/master/lib/std/hash_map.zig[hash_map.zig] link:https://github.com/ziglang/zig/tree/master/lib/std/hash/crc.zig[crc.zig] link:https://github.com/ziglang/zig/tree/master/lib/std/Progress.zig[Progress.zig]

[[learning-goals]]
== Learning Goals

* Assemble a ZIP archive from scratch by writing Local File Headers, the Central Directory, and the End of Central Directory record in the correct order while honoring size and offset constraints.
* Capture deterministic integrity metrics (CRC32, SHA-256) alongside the bundle so continuous integration can validate both structure and content on every run. link:https://github.com/ziglang/zig/tree/master/lib/std/crypto.zig[crypto.zig]
* Surface analyst-friendly progress messages that stay scriptable by disabling the animated renderer and emitting plain-text checkpoints with `std.Progress`.

[[pipeline-design]]
== Designing the pipeline

The workflow runs in three phases: seed sample files, build an archive, and extract plus verify. Each phase increments the root progress node, producing deterministic console summaries that double as acceptance criteria. All filesystem operations occur under a temporary directory managed by `std.testing.tmpDir`, keeping the real workspace clean. xref:47__time-logging-and-progress.adoc[47] link:https://github.com/ziglang/zig/tree/master/lib/std/testing.zig[testing.zig]

For archival metadata, we reuse the same relative paths when writing headers and when later validating the extracted files. Storing the CRC32 and byte count per path inside a `StringHashMap` gives us a straightforward way to diff expectations against actual outputs after extraction.

[[archive-assembly]]
== Archive assembly

Because Zig 0.15.2 ships a ZIP reader but not a writer, we build the archive in memory using an `ArrayList(u8)`, appending each component in sequence: Local File Header, filename, file bytes. Every header field is written with explicit little-endian helpers so the result is portable across architectures. Once the payloads land in the blob, we append the Central Directory (one record per file) followed by the End of Central Directory record, mirroring the structures defined in the PKWARE APPNOTE and encoded in `std.zip`. link:https://github.com/ziglang/zig/tree/master/lib/std/array_list.zig[array_list.zig] link:https://github.com/ziglang/zig/tree/master/lib/std/fmt.zig[fmt.zig]

While writing headers we ensure sizes and offsets fit in 32-bit fields (sticking to the classic ZIP subset) and duplicate the filename once into the map so we can free resources deterministically later. After the archive image is complete, we persist it to disk and compute a SHA-256 digest for downstream regressionsâ€”the digest is rendered with `std.fmt.bytesToHex` so it can be compared inline without any extra tooling.

[[extraction-and-verification]]
== Extraction and verification

Extraction reuses the standard library iterator, which walks through each Central Directory record and hands the data stream to `std.zip.Entry.extract`; we normalize the root folder name through `std.zip.Diagnostics` so we can surface it to the caller. After each file lands on disk, we compute CRC32 again and compare the byte count against the recorded expectation. Any mismatch fails the program immediately, making it safe to embed in CI pipelines or deployment hooks.

`std.Progress` nodes drive the console output: the root node tracks the three high-level stages, while child nodes count through the file list during seeding, building, and verification. Because printing is disabled, the final messages are ordinary text lines (rendered via a buffered stdout writer) that can be diffed verbatim in automated tests. xref:47__time-logging-and-progress.adoc[47]

[[code-listing]]
== End-to-end implementation

[source,zig]
----
include::{sourcedir}/54__project-zip-unzip-with-progress/zip_progress_pipeline.zig[]
----

.Run
[source,shell]
----
$ zig run zip_progress_pipeline.zig
----

.Output
[source,shell]
----
[1/3] seeded samples -> files=4, bytes=250
[2/3] built archive -> bytes=716
    sha256=4a13a3dc1e6ef90c252b0cc797ff14456aa28c670cafbc9d27a025b0079b05d5
[3/3] extracted + verified -> files=4, bytes=250, root=input
----

The verification step intentionally duplicates the extracted root string when diagnostics discover a common prefix; the summary frees that buffer afterward to keep the general-purpose allocator clean. This mirrors good hygiene for CLI utilities that stream large archives through temporary directories. xref:52__debug-and-valgrind.adoc[52]

[[notes-caveats]]
== Notes & Caveats

* The writer sticks to the classic (non-Zip64) subset; once files exceed 4 GiB you must upgrade the headers and extra fields, or delegate to a dedicated ZIP library. xref:44__collections-and-algorithms.adoc[44]
* Progress nodes are nested but printing is disabled; if you want live TTY updates, drop `.disable_printing = true` and let the renderer clear frames. Remember that doing so sacrifices determinism in captured logs. xref:47__time-logging-and-progress.adoc[47]
* CRC32 confirms integrity but not authenticity. Combine the SHA-256 digest with a signature or attach the archive to a `zig build` step for reproducible deployment pipelines. xref:39__performance-and-inlining.adoc[39]

[[exercises]]
== Exercises

* Extend the builder to emit Zip64 records when any file crosses the 4 GiB boundary. Keep the legacy path for small bundles and write regression tests that validate both. xref:33__c-interop-import-export-abi.adoc[33]
* Replace the in-memory blob with a streaming writer that flushes to disk in chunks; compare throughput and memory consumption under `perf` or `zig build test` with large synthetic files. xref:41__cross-compilation-and-wasm.adoc[41]
* Add a command-line flag that accepts an ignore list (glob patterns) before archiving, then report the exact number of skipped files alongside the existing totals. xref:36__style-and-best-practices.adoc[36] link:https://github.com/ziglang/zig/tree/master/lib/std/fs/Dir.zig[Dir.zig]

[[caveats-alternatives]]
== Caveats, alternatives, edge cases

* Streaming archives straight to stdout is great for pipelines but makes verification trickier; consider writing to a temporary file first so you can re-open it for checksums before shipping it onward. xref:28__filesystem-and-io.adoc[28] link:https://github.com/ziglang/zig/tree/master/lib/std/fs/File.zig[File.zig]
* ZIP encryption is intentionally out of scope. If you need confidentiality, wrap the resulting file with `std.crypto` primitives or switch to formats like encrypted tarballs with age or minisign. xref:45__text-formatting-and-unicode.adoc[45]
* For multi-gigabyte corpora, read inputs in chunks and update CRC32 incrementally rather than calling `readToEndAlloc`; otherwise the temporary allocator will balloon. xref:10__allocators-and-memory-management.adoc[10]