<?xml version="1.0" encoding="UTF-8"?>
<?asciidoc-toc?>
<?asciidoc-numbered?>
<book xmlns="http://docbook.org/ns/docbook" xmlns:xl="http://www.w3.org/1999/xlink" version="5.0" xml:lang="en">
<info>
<title>Project</title>
<subtitle>Top-K Word Frequency Analyzer</subtitle>
<date>2025-11-15</date>
<copyright>
<holder>zigbook</holder>
</copyright>
</info>
<chapter xml:id="overview">
<title>Overview</title>
<simpara>The debugging chapter introduced tools for explaining <emphasis role="strong">why</emphasis> a program misbehaves.<link xl:href="52__debug-and-valgrind.xml">52</link> This project leans on similar discipline to build a deterministic text analytics utility: feed it a log excerpt, collect the most frequent tokens, and emit timing data for each phase. We will combine the tokenization helpers from <literal>std.mem</literal>, hashed collections from <literal>std</literal>, the <literal>heap</literal> sorter, and the <literal>Timer</literal> API to produce reproducible rankings with measured costs.<link xl:href="https://github.com/ziglang/zig/tree/master/lib/std/mem.zig">mem.zig</link> <link xl:href="https://github.com/ziglang/zig/tree/master/lib/std/hash_map.zig">hash_map.zig</link> <link xl:href="https://github.com/ziglang/zig/tree/master/lib/std/sort.zig">sort.zig</link> <link xl:href="https://github.com/ziglang/zig/tree/master/lib/std/time.zig">time.zig</link></simpara>
</chapter>
<chapter xml:id="learning-goals">
<title>Learning Goals</title>
<itemizedlist>
<listitem>
<simpara>Build an end-to-end I/O pipeline that reads a corpus, normalizes text, and accumulates counts with <literal>std.StringHashMap</literal> and <literal>std.ArrayList</literal>.<link xl:href="https://github.com/ziglang/zig/tree/master/lib/std/array_list.zig">array_list.zig</link></simpara>
</listitem>
<listitem>
<simpara>Rank frequencies deterministically using an explicit comparator that resolves ties without depending on hash map iteration order.</simpara>
</listitem>
<listitem>
<simpara>Capture per-phase timings with <literal>std.time.Timer</literal> to validate regressions and communicate performance expectations.</simpara>
</listitem>
</itemizedlist>
</chapter>
<chapter xml:id="pipeline-design">
<title>Designing the Pipeline</title>
<simpara>Our analyzer accepts an optional file path and an optional <literal>k</literal> parameter (<literal>top k</literal> tokens); both default to a bundled corpus and <literal>5</literal>, respectively. We read the entire file into memory for simplicity, but the normalization and counting loop is written to operate linearly so it can be adapted to stream chunks later. A <literal>GeneralPurposeAllocator</literal> backs all dynamic structures, and an arena-friendly workflow (duplicating strings only on first sight) keeps allocations proportional to the vocabulary size.<link xl:href="https://github.com/ziglang/zig/tree/master/lib/std/heap.zig">heap.zig</link> <link xl:href="https://github.com/ziglang/zig/tree/master/lib/std/process.zig">process.zig</link></simpara>
<simpara>Tokenization happens with <literal>std.mem.tokenizeAny</literal>, configured with a conservative separator set that trims whitespace, punctuation, and markup characters. Each token is lowercased in a reusable <literal>std.ArrayList(u8)</literal> before attempting insertion into the map; if the token already exists, only the count increments, keeping temporary allocations bounded.</simpara>
</chapter>
<chapter xml:id="count-and-rank">
<title>Counting and Ranking</title>
<simpara>The complete utility demonstrates StringHashMap, ArrayList, sorting, and timing side by side.</simpara>
<programlisting language="zig" linenumbering="unnumbered">Unresolved directive in 53__project-top-k-word-frequency-analyzer.adoc - include::example$chapters-data/code/53__project-top-k-word-frequency-analyzer/topk_word_frequency.zig[]</programlisting>
<formalpara>
<title>Run</title>
<para>
<programlisting language="shell" linenumbering="unnumbered">$ zig run chapters-data/code/53__project-top-k-word-frequency-analyzer/topk_word_frequency.zig</programlisting>
</para>
</formalpara>
<formalpara>
<title>Output</title>
<para>
<programlisting language="shell" linenumbering="unnumbered">source -&gt; chapters-data/code/53__project-top-k-word-frequency-analyzer/sample_corpus.txt
tokens -&gt; 102, unique -&gt; 86
top 5 words:
   1. the -&gt; 6
   2. a -&gt; 3
   3. and -&gt; 3
   4. are -&gt; 2
   5. latency -&gt; 2
timings (ns): read=284745, tokenize=3390822, sort=236725</programlisting>
</para>
</formalpara>
<simpara><literal>std.StringHashMap</literal> stores the canonical lowercase spellings, and a separate <literal>std.ArrayList</literal> collects the final <literal>(word, count)</literal> pairs for sorting. We choose <literal>std.sort.heap</literal> because it is deterministic, has no allocator dependencies, and performs well on small datasets; the comparator sorts primarily by descending count and secondarily by lexical ordering to keep ties stable. This is important when rerunning analyses across runs or machines—the field team can diff resulting CSVs without surprises.</simpara>
</chapter>
<chapter xml:id="timing-and-reproducibility">
<title>Timing and Reproducibility</title>
<simpara>A single <literal>Timer</literal> instance measures three phases: file ingestion, tokenization, and sorting. We call <literal>lap()</literal> after each phase to reset the zero point while recording elapsed nanoseconds, making it easy to spot which step dominates. Because the analyzer normalizes case and uses deterministic sorting, the output for a given corpus remains identical across runs, allowing timing deltas to be attributed to hardware or toolchain changes rather than nondeterministic ordering.</simpara>
<simpara>For regressions, rerun with a larger <literal>k</literal> or a different corpus:</simpara>
<programlisting language="shell" linenumbering="unnumbered">$ zig run chapters-data/code/53__project-top-k-word-frequency-analyzer/topk_word_frequency.zig -- chapters-data/code/53__project-top-k-word-frequency-analyzer/sample_corpus.txt 10</programlisting>
<simpara>The optional arguments let you keep the binary scriptable—drop it into CI, compare output artifacts, and alert when the timing budget changes by more than a threshold. When integrating into bigger systems, the map-building loop can be swapped to stream from <literal>stdin</literal> or a TCP socket while preserving the same deterministic ranking rules.<link xl:href="https://github.com/ziglang/zig/tree/master/lib/std/fs/File.zig">File.zig</link></simpara>
</chapter>
<chapter xml:id="notes-caveats">
<title>Notes &amp; Caveats</title>
<itemizedlist>
<listitem>
<simpara><literal>StringHashMap</literal> does not free stored keys automatically; this sample releases them explicitly before dropping the map to keep the general-purpose allocator leak checker happy.</simpara>
</listitem>
<listitem>
<simpara>The tokenizer is ASCII-focused. For full Unicode segmentation, pair the pipeline with <literal>std.unicode.ScalarIterator</literal> or integrate ICU bindings.<link xl:href="45__text-formatting-and-unicode.xml">45</link></simpara>
</listitem>
<listitem>
<simpara>Reading the entire corpus into memory simplifies the tutorial but may not suit multi-gigabyte logs. Swap <literal>readFileAlloc</literal> for chunked <literal>readAll</literal> loops or memory-mapped files when scaling.<link xl:href="28__filesystem-and-io.xml">28</link></simpara>
</listitem>
</itemizedlist>
</chapter>
<chapter xml:id="exercises">
<title>Exercises</title>
<itemizedlist>
<listitem>
<simpara>Emit the report as JSON by serializing the sorted slice, then compare diff friendliness with the textual version.<link xl:href="32__project-http-json-client.xml">32</link> <link xl:href="https://github.com/ziglang/zig/tree/master/lib/std/json.zig">json.zig</link></simpara>
</listitem>
<listitem>
<simpara>Replace the single-threaded analyzer with a two-phase pipeline: shard tokens across threads, then merge hash maps before sorting. Measure the benefit using <literal>Timer</literal> and summarize the scaling.<link xl:href="29__threads-and-atomics.xml">29</link></simpara>
</listitem>
<listitem>
<simpara>Add a <literal>--stopwords</literal> option that loads a newline-delimited ignore list, removes those tokens before counting, and reports how many candidates were filtered out.<link xl:href="36__style-and-best-practices.xml">36</link></simpara>
</listitem>
</itemizedlist>
</chapter>
<chapter xml:id="caveats-alternatives">
<title>Caveats, Alternatives, Edge Cases</title>
<itemizedlist>
<listitem>
<simpara>For streaming environments, consider <literal>std.PriorityQueue</literal> to maintain the top <literal>k</literal> incrementally instead of recording the entire histogram before sorting.<link xl:href="https://github.com/ziglang/zig/tree/master/lib/std/priority_queue.zig">priority_queue.zig</link></simpara>
</listitem>
<listitem>
<simpara>If performance requirements outgrow heap sort, experiment with <literal>std.sort.pdq</literal> or bucket-based approaches while keeping the deterministic comparator contract intact.</simpara>
</listitem>
<listitem>
<simpara>To support multi-locale text, layer in normalization (NFC/NFKC) and use Unicode-aware casing helpers; the comparator may need locale-specific collation to keep results intuitive.<link xl:href="45__text-formatting-and-unicode.xml">45</link></simpara>
</listitem>
</itemizedlist>
</chapter>
</book>