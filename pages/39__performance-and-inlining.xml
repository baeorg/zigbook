<?xml version="1.0" encoding="UTF-8"?>
<?asciidoc-toc?>
<?asciidoc-numbered?>
<book xmlns="http://docbook.org/ns/docbook" xmlns:xl="http://www.w3.org/1999/xlink" version="5.0" xml:lang="en">
<info>
<title>Performance &amp; Inlining</title>
<date>2025-11-15</date>
<copyright>
<holder>zigbook</holder>
</copyright>
</info>
<chapter xml:id="overview">
<title>Overview</title>
<simpara>Our CLI survey set the stage for disciplined experimentation. <link xl:href="38__zig-cli-deep-dive.xml">38</link> Now we focus on how Zig translates those command-line toggles into machine-level behavior. Semantic inlining, call modifiers, and explicit SIMD all give you levers to shape hot paths—provided you measure carefully and respect the compiler&#8217;s defaults. <link xl:href="https://ziglang.org/documentation/master/#inline-fn">#inline fn</link></simpara>
<simpara>The next chapter formalizes that measurement loop by layering profiling and hardening workflows on top. <link xl:href="40__profiling-optimization-hardening.xml">40</link></simpara>
</chapter>
<chapter xml:id="learning-goals">
<title>Learning Goals</title>
<itemizedlist>
<listitem>
<simpara>Force or forbid inlining when compile-time semantics must win over heuristics.</simpara>
</listitem>
<listitem>
<simpara>Sample hot loops with <literal>@call</literal> and <literal>std.time.Timer</literal> to compare build modes.</simpara>
</listitem>
<listitem>
<simpara>Use <literal>@Vector</literal> math as a bridge to portable SIMD before reaching for target-specific intrinsics.</simpara>
</listitem>
</itemizedlist>
<simpara><link xl:href="https://ziglang.org/documentation/master/#call">#call</link>, <link xl:href="https://github.com/ziglang/zig/tree/master/lib/std/time/Timer.zig">Timer.zig</link>, <link xl:href="https://ziglang.org/documentation/master/#vectors">#vectors</link></simpara>
</chapter>
<chapter xml:id="inline-semantics">
<title>Semantic Inlining vs Optimizer Heuristics</title>
<simpara>Zig&#8217;s <literal>inline</literal> keyword changes evaluation rules rather than hinting at the optimizer: compile-time known arguments become compile-time constants, allowing you to generate types or precompute values that ordinary calls would defer to runtime.</simpara>
<simpara>Inline functions restrict the compiler&#8217;s freedom, so reach for them only when semantics matter—propagating <literal>comptime</literal> data, improving debugging, or satisfying real benchmarks.</simpara>
<section xml:id="_understanding_optimization_modes">
<title>Understanding Optimization Modes</title>
<simpara>Before exploring inlining behavior, it&#8217;s important to understand the optimization modes that affect how the compiler treats your code. The following diagram shows the optimization configuration:</simpara>
<literallayout class="monospaced">graph TB
    subgraph "Optimization"
        OPTIMIZE["Optimization Settings"]
        OPTIMIZE --&gt; OPTMODE["optimize_mode: OptimizeMode&lt;br/&gt;Debug, ReleaseSafe, ReleaseFast, ReleaseSmall"]
        OPTIMIZE --&gt; LTO["lto: bool&lt;br/&gt;Link Time Optimization"]
    end</literallayout>
<simpara>Zig provides four distinct optimization modes, each making different tradeoffs between safety, speed, and binary size. <emphasis role="strong">Debug</emphasis> mode disables optimizations and keeps full runtime safety checks, making it ideal for development and debugging. The compiler preserves stack frames, emits symbol information, and never inlines functions unless semantically required. <emphasis role="strong">ReleaseSafe</emphasis> enables optimizations while retaining all safety checks (bounds checking, integer overflow detection, etc.), balancing performance with error detection. <emphasis role="strong">ReleaseFast</emphasis> maximizes speed by disabling runtime safety checks and enabling aggressive optimizations including heuristic inlining. This is the mode used in the benchmarks throughout this chapter. <emphasis role="strong">ReleaseSmall</emphasis> prioritizes binary size over speed, often disabling inlining entirely to reduce code duplication.</simpara>
<simpara>Additionally, <emphasis role="strong">Link Time Optimization (LTO)</emphasis> can be enabled independently via <literal>-flto</literal>, allowing the linker to perform whole-program optimization across compilation units. When benchmarking inlining behavior, these modes dramatically affect results: <literal>inline</literal> functions behave identically across modes (semantic guarantee), but heuristic inlining in ReleaseFast may inline functions that Debug or ReleaseSmall would leave as calls. The chapter&#8217;s examples use <literal>-OReleaseFast</literal> to showcase optimizer behavior, but you should test across modes to understand the full performance spectrum.</simpara>
</section>
<section xml:id="inline-fibonacci">
<title>Example: compile-time math with inline functions</title>
<simpara><literal>inline</literal> recursion lets us bake small computations into the binary while leaving a fallback runtime path for larger inputs. The <literal>@call</literal> builtin provides a direct handle to evaluate call sites at compile time when arguments are available.</simpara>
<programlisting language="zig" linenumbering="unnumbered">Unresolved directive in 39__performance-and-inlining.adoc - include::example$chapters-data/code/39__performance-and-inlining/01_inline_semantics.zig[]</programlisting>
<formalpara>
<title>Run</title>
<para>
<programlisting language="shell" linenumbering="unnumbered">$ zig test 01_inline_semantics.zig</programlisting>
</para>
</formalpara>
<formalpara>
<title>Output</title>
<para>
<programlisting language="shell" linenumbering="unnumbered">All 3 tests passed.</programlisting>
</para>
</formalpara>
<tip>
<simpara>The <literal>.compile_time</literal> modifier fails if the callee touches runtime-only state. Wrap such experiments in <literal>comptime</literal> blocks first, then add runtime tests so release builds remain covered.</simpara>
</tip>
</section>
</chapter>
<chapter xml:id="call-modifiers">
<title>Directing Calls for Measurement</title>
<simpara>Zig 0.15.2&#8217;s self-hosted backends reward accurate microbenchmarks. They can deliver dramatic speedups when paired with the new threaded code generation pipeline. <link xl:href="https://ziglang.org/download/0.15.1/release-notes.html#threaded-codegen">v0.15.2</link></simpara>
<simpara>Use <literal>@call</literal> modifiers to compare inline, default, and never-inline dispatches without refactoring your call sites.</simpara>
<section xml:id="call-benchmark">
<title>Example: comparing call modifiers under ReleaseFast</title>
<simpara>This benchmark pins the optimizer (<literal>-OReleaseFast</literal>) while we toggle call modifiers. Every variant produces the same result, but the timing highlights how <literal>never_inline</literal> can balloon hot loops when function call overhead dominates.</simpara>
<programlisting language="zig" linenumbering="unnumbered">Unresolved directive in 39__performance-and-inlining.adoc - include::example$chapters-data/code/39__performance-and-inlining/03_call_benchmark.zig[]</programlisting>
<formalpara>
<title>Run</title>
<para>
<programlisting language="shell" linenumbering="unnumbered">$ zig run 03_call_benchmark.zig -OReleaseFast</programlisting>
</para>
</formalpara>
<formalpara>
<title>Output</title>
<para>
<programlisting language="shell" linenumbering="unnumbered">optimize-mode=ReleaseFast iterations=5000000
auto call   : 161394 ns
always_inline: 151745 ns
never_inline : 2116797 ns</programlisting>
</para>
</formalpara>
<note>
<simpara>Performing the same run under <literal>-OReleaseSafe</literal> makes the gap larger because additional safety checks amplify the per-call overhead. <link xl:href="https://ziglang.org/download/0.15.1/release-notes.html#x86-backend">v0.15.2</link> Use <literal>zig run --time-report</literal> from the previous chapter when you want compiler-side attribution for slow code paths. <link xl:href="38__zig-cli-deep-dive.xml#cli-webui">38</link></simpara>
</note>
</section>
</chapter>
<chapter xml:id="vectorization">
<title>Portable Vectorization with @Vector</title>
<simpara>When the compiler cannot infer SIMD usage on its own, <literal>@Vector</literal> types offer a portable shim that respects safety checks and fallback scalar execution. Paired with <literal>@reduce</literal>, you can express horizontal reductions without writing target-specific intrinsics. <link xl:href="https://ziglang.org/documentation/master/#reduce">#reduce</link></simpara>
<section xml:id="vector-example">
<title>Example: SIMD-friendly dot product</title>
<simpara>The scalar and vectorized versions produce identical results. Profiling determines whether the extra vector plumbing pays off on your target.</simpara>
<programlisting language="zig" linenumbering="unnumbered">Unresolved directive in 39__performance-and-inlining.adoc - include::example$chapters-data/code/39__performance-and-inlining/02_vector_reduction.zig[]</programlisting>
<formalpara>
<title>Run</title>
<para>
<programlisting language="shell" linenumbering="unnumbered">$ zig test 02_vector_reduction.zig</programlisting>
</para>
</formalpara>
<formalpara>
<title>Output</title>
<para>
<programlisting language="shell" linenumbering="unnumbered">All 1 tests passed.</programlisting>
</para>
</formalpara>
<tip>
<simpara>Once you start mixing vectors and scalars, use <literal>@splat</literal> to lift constants and avoid the implicit casts forbidden by the vector rules.</simpara>
</tip>
</section>
</chapter>
<chapter xml:id="notes-caveats">
<title>Notes &amp; Caveats</title>
<itemizedlist>
<listitem>
<simpara>Inline recursion counts against the compile-time branch quota. Raise it with <literal>@setEvalBranchQuota</literal> only when measurements prove the extra compile-time work is worthwhile. <link xl:href="https://ziglang.org/documentation/master/#setevalbranchquota">#setevalbranchquota</link></simpara>
</listitem>
<listitem>
<simpara>Switching between <literal>@call(.always_inline, &#8230;&#8203;)</literal> and the <literal>inline</literal> keyword matters: the former applies to a single site, whereas <literal>inline</literal> modifies the callee definition and every future call.</simpara>
</listitem>
<listitem>
<simpara>Vector lengths other than powers of two may fall back to scalar loops on some targets. Capture the generated assembly with <literal>zig build-exe -femit-asm</literal> before banking on a win.</simpara>
</listitem>
</itemizedlist>
<section xml:id="_code_generation_features_affecting_performance">
<title>Code Generation Features Affecting Performance</title>
<simpara>Beyond optimization modes, several code generation features affect runtime performance and debuggability. Understanding these flags helps you reason about performance tradeoffs:</simpara>
<literallayout class="monospaced">graph TB
    subgraph "Code Generation Features"
        Features["Feature Flags"]

        Features --&gt; UnwindTables["unwind_tables: bool"]
        Features --&gt; StackProtector["stack_protector: bool"]
        Features --&gt; StackCheck["stack_check: bool"]
        Features --&gt; RedZone["red_zone: ?bool"]
        Features --&gt; OmitFramePointer["omit_frame_pointer: bool"]
        Features --&gt; Valgrind["valgrind: bool"]
        Features --&gt; SingleThreaded["single_threaded: bool"]

        UnwindTables --&gt; EHFrame["Generate .eh_frame&lt;br/&gt;for exception handling"]

        StackProtector --&gt; CanaryCheck["Stack canary checks&lt;br/&gt;buffer overflow detection"]

        StackCheck --&gt; ProbeStack["Stack probing&lt;br/&gt;prevents overflow"]

        RedZone --&gt; RedZoneSpace["Red zone optimization&lt;br/&gt;(x86_64, AArch64)"]

        OmitFramePointer --&gt; NoFP["Omit frame pointer&lt;br/&gt;for performance"]

        Valgrind --&gt; ValgrindSupport["Valgrind client requests&lt;br/&gt;for memory debugging"]

        SingleThreaded --&gt; NoThreading["Assume single-threaded&lt;br/&gt;enable optimizations"]
    end</literallayout>
<simpara>The <emphasis role="strong">omit_frame_pointer</emphasis> flag is particularly relevant for performance work: when enabled (typical in ReleaseFast), the compiler frees the frame pointer register (RBP on x86_64, FP on ARM) for general use, improving register allocation and enabling more aggressive optimizations. However, this makes stack unwinding harder. Debuggers and profilers may produce incomplete or missing stack traces.</simpara>
<simpara>The <emphasis role="strong">red_zone</emphasis> optimization (x86_64 and AArch64 only) allows functions to use 128 bytes below the stack pointer without adjusting RSP, reducing prologue/epilogue overhead in leaf functions. <emphasis role="strong">Stack protection</emphasis> adds canary checks to detect buffer overflows but adds runtime cost. This is why ReleaseFast disables it. <emphasis role="strong">Stack checking</emphasis> instruments functions to probe the stack and prevent overflow, useful for deep recursion but costly. <emphasis role="strong">Unwind tables</emphasis> generate <literal>.eh_frame</literal> sections for exception handling and debugger stack walks. Debug mode always includes them; release modes may omit them for size.</simpara>
<simpara>When the exercises suggest measuring allocator hot paths with <literal>@call(.never_inline, &#8230;&#8203;)</literal>, these flags explain why Debug mode shows better stack traces (frame pointers preserved) at the cost of slower execution (extra instructions, no register optimization). Performance-critical code should benchmark with ReleaseFast but validate correctness with Debug to catch issues the optimizer might hide.</simpara>
</section>
</chapter>
<chapter xml:id="exercises">
<title>Exercises</title>
<itemizedlist>
<listitem>
<simpara>Add a <literal>--mode</literal> flag to the benchmark program so you can flip between Debug, ReleaseSafe, and ReleaseFast runs without editing the code. <link xl:href="38__zig-cli-deep-dive.xml#cli-run-summary">38</link></simpara>
</listitem>
<listitem>
<simpara>Extend the dot-product example with a remainder loop that handles slices whose lengths are not multiples of four. Measure the crossover point where SIMD still wins.</simpara>
</listitem>
<listitem>
<simpara>Experiment with <literal>@call(.never_inline, &#8230;&#8203;)</literal> on allocator hot paths from Chapter 10 to confirm whether improved stack traces in Debug are worth the runtime cost. <link xl:href="10__allocators-and-memory-management.xml">10</link></simpara>
</listitem>
</itemizedlist>
</chapter>
<chapter xml:id="caveats-alternatives-edge-cases">
<title>Alternatives &amp; Edge Cases:</title>
<itemizedlist>
<listitem>
<simpara>Microbenchmarks that run inside <literal>zig run</literal> share the compilation cache. Warm the cache with a dummy run before comparing timings to avoid skew. <link xl:href="ZIG_DEEP_WIKI.md#entry-points-and-command-structure">#entry points and command structure</link></simpara>
</listitem>
<listitem>
<simpara>The self-hosted x86 backend is fast but not perfect. Fall back to <literal>-fllvm</literal> if you notice miscompilations while exploring aggressive inline patterns.</simpara>
</listitem>
<listitem>
<simpara>ReleaseSmall often disables inlining entirely to save size. When you need both tiny binaries and tuned hot paths, isolate the hot functions and call them from a ReleaseFast-built shared library.</simpara>
</listitem>
</itemizedlist>
</chapter>
</book>