////
changes: ["New project chapter: Generic Priority Queue (verbose, in-depth)", "Added three runnable demos covering comparators, policy contexts, and top-K analytics"]
examples_compile: yes
keywords: ["priority queue", "comparator", "scheduler", "TopK", "std.PriorityQueue"]
last_updated: 2025-11-03
last_verified: 2025-11-03
next_chapter: "19__modules-and-imports-root-builtin-discovery"
open_questions: []
previous_chapter: "17__generic-apis-and-type-erasure"
status: draft
xref_complete: true
////

= Project: Generic Priority Queue
:chapter-number: 18
:chapter-slug: project-generic-priority-queue
:doctype: book
:embedded:
:experimental:
:icons: font
:partnums:
:pygments-linenums-mode: inline
:pygments-style: manni
:safe-mode-level: 0
:sectanchors:
:sectids:
:sectlinks:
:source-highlighter: pygments
:sourcedir: example$chapters-data/code
:webfonts:
:xrefstyle: short
:zig-version: 0.15.2
:linkcss:
:stylesdir: styles
:stylesheet: zigbook.css

[[overview]]
== Overview

Generic APIs let us describe capabilities at compile time; priority queues are where those capabilities meet the realities of time-sensitive scheduling. In this project, we wrap `std.PriorityQueue` with rich comparators and context-aware policies that can be tested and tuned without sacrificing zero-cost abstractions. See xref:17__generic-apis-and-type-erasure.adoc[17] and link:https://github.com/ziglang/zig/tree/master/lib/std/priority_queue.zig[priority_queue.zig].

We’ll build three artefacts: a foundational dispatcher that encodes ordering rules in a comparator, a fairness simulator that reuses the same queue while changing policy context, and an analytics wrapper that tracks the top offenders in a stream. Along the way, we revisit allocator choices, weighing strategies for draining, retuning, and introspecting heaps. See xref:10__allocators-and-memory-management.adoc[10] and link:https://github.com/ziglang/zig/tree/master/lib/std/sort.zig[sort.zig].

[[learning-goals]]
== Learning Goals

* Translate business rules into compile-time comparator contracts that drive `std.PriorityQueue` ordering.
* Model dynamic scheduling heuristics using the queue’s `Context` parameter while keeping memory churn predictable. xref:10__allocators-and-memory-management.adoc[10]
* Derive streaming analytics (top-K, rolling statistics) from the same heap without copy-pasting logic or sacrificing stability. xref:45__text-formatting-and-unicode.adoc[47]

[[architect-core]]
== Architecting a reusable queue core

The priority queue API accepts a value type, a user-defined context, and a comparator that returns `std.math.Order`. That one function decides which element is bubbled to the front, so we’ll treat it as a contract backed by tests.

[[core-comparator-contract]]
=== Comparator design as API surface

Our first example builds a simple build-and-release dispatcher. Urgency is the primary key; submission time breaks ties so that we avoid starving older tasks. The comparator is a pure function, invoked entirely at compile time when the queue type is instantiated, yet it is expressive enough to capture nuanced ordering logic. See link:https://github.com/ziglang/zig/tree/master/lib/std/math.zig[math.zig].

[source,zig]
----
include::{sourcedir}/18__project-generic-priority-queue/task_queue_basics.zig[]
----

.Run
[source,shell]
----
$ zig run task_queue_basics.zig
----

.Output
[source,shell]
----
Dispatch order:
  - compile pointer.zig (urgency 0)
  - run tests (urgency 1)
  - prepare changelog (urgency 1)
  - deploy preview (urgency 2)
----

TIP: Because the comparator returns `std.math.Order`, we can layer in secondary keys without changing the queue type; the heap simply obeys the contract you encode.

[[core-growth-allocation]]
=== Growth and allocation strategy

Every call to `add` may reallocate if the underlying slice needs more capacity. For hot paths, reserve with `ensureUnusedCapacity` or initialize from a pre-sized slice, then drain to amortize allocations. The queue’s `deinit` is cheap so long as you make allocator lifetimes explicit, mirroring the memory hygiene practices from our allocator deep dive. xref:10__allocators-and-memory-management.adoc[10]

[[policy-driven]]
== Policy-driven reprioritization

Next, we feed richer data into the same queue: service requests with SLAs, time-of-day context, and VIP hints. The queue itself is agnostic; all nuance lives in the policy structure and comparator. This design keeps the heap reusable even as we layer on fairness rules. xref:17__generic-apis-and-type-erasure.adoc[17]

[[policy-aging]]
=== Aging and VIP weighting

The comparator computes a scalar “score” by measuring slack (time remaining until deadline), multiplying overdue requests to escalate them, and subtracting a VIP bonus. Because `Context` is just a struct, the policy is compiled into the queue and can be swapped by constructing a new instance with different weights. We forward-declare helper functions to keep the comparator readable and testable. 

[[policy-scenarios]]
=== Simulating operating modes

We run two scenarios: mid-shift triage and late escalation. The only difference is the policy struct we pass to `init`; everything else (tasks, queue type) stays the same. The printed order shows how overdue multiplication and VIP boosts change the pop sequence.

[source,zig]
----
include::{sourcedir}/18__project-generic-priority-queue/sla_fairness.zig[]
----

.Run
[source,shell]
----
$ zig run sla_fairness.zig
----

.Output
[source,shell]
----
Mid-shift triage (now=350ms)
  -> INC-993 score=-20 deadline=520 vip=true
  -> INC-511 score=95 deadline=400 vip=false
  -> INC-742 score=140 deadline=460 vip=false
  -> INC-482 score=270 deadline=500 vip=false

Escalation window (now=520ms)
  -> INC-511 score=-435 deadline=400 vip=false
  -> INC-742 score=-210 deadline=460 vip=false
  -> INC-993 score=-40 deadline=520 vip=true
  -> INC-482 score=40 deadline=500 vip=false
----

IMPORTANT: Changing policy after enqueuing existing items requires rebuilding the heap—drain into a slice, mutate the policy, then reinsert or call `fromOwnedSlice` to re-heapify under the new comparator. xref:10__allocators-and-memory-management.adoc[10]

[[analytics-topk]]
== Analytics and top-K reporting

Priority queues are also excellent rolling aggregates. By keeping the “worst” elements in the heap and trimming aggressively, we can maintain a top-K view of latency spikes with minimal overhead. Sorting the current heap snapshot lets us render results directly for dashboards or logs. xref:45__text-formatting-and-unicode.adoc[47]

[[analytics-wrapper]]
=== A composable `TopK` wrapper

`TopK` wraps `std.PriorityQueue` and uses the comparator to form a min-heap of scores. Every insert calls `remove` when the heap exceeds the limit, ensuring we keep only the highest scorers. The `snapshotDescending` helper copies the heap into a scratch buffer and sorts it with `std.sort.heap`, leaving the queue ready for further inserts. xref:17__generic-apis-and-type-erasure.adoc[17]

[source,zig]
----
include::{sourcedir}/18__project-generic-priority-queue/topk_latency.zig[]
----

.Run
[source,shell]
----
$ zig run topk_latency.zig
----

.Output
[source,shell]
----
Top latency offenders (descending by score):
   1. /v1/ledger   latency=420ms payload=540B score=417.30
   2. /v1/ledger   latency=362ms payload=480B score=359.60
   3. /v1/payments latency=305ms payload=1500B score=297.50
   4. /v1/users    latency=275ms payload=980B score=270.10
   5. /v1/orders   latency=210ms payload=1200B score=204.00
----

NOTE: Snapshotting copies the heap so that future inserts remain cheap; reuse a scratch allocator or arena for high-volume telemetry jobs to avoid fragmenting long-lived heaps. xref:10__allocators-and-memory-management.adoc[10]

[[closing-loop]]
=== From queues to module boundaries

We now have reusable queue wrappers that can live in their own module. The next chapter formalizes that step, showing how to surface the queue as a package-level module and expose policies through `@import` boundaries. xref:19__modules-and-imports-root-builtin-discovery.adoc[19]

[[notes-caveats]]
== Notes & Caveats

* Define comparators in a dedicated helper so they can be unit-tested independently and reused across queue instances. xref:13__testing-and-leak-detection.adoc[13]
* Policy structs are value types—change detection means rebuilding the heap or creating a new queue; otherwise, your ordering no longer matches the comparator’s assumptions.
* Copying heap contents for reporting allocates memory; recycle buffers or use arenas when integrating with telemetry services to keep GC-less Zig code predictable. xref:10__allocators-and-memory-management.adoc[10]

[[exercises]]
== Exercises

* Extend the dispatcher to respect “batch size” hints by tallying cumulative runtime in the comparator; add a test that asserts fairness across mixed priorities. xref:13__testing-and-leak-detection.adoc[13]
* Modify the SLA simulator to write audit entries using `std.log` and compare the output against expectations under multiple policies. link:https://github.com/ziglang/zig/tree/master/lib/std/log.zig[log.zig]
* Teach the `TopK` wrapper to return both the snapshot and the aggregate average; consider how you would expose that through an asynchronous metrics hook. xref:45__text-formatting-and-unicode.adoc[47]

[[caveats-alternatives-edge-cases]]
== Alternatives & Edge Cases

* If you need stable ordering for items with identical scores, wrap the payload in a struct that stores a monotonically increasing sequence number and include it in the comparator. 
* For extremely large queues, consider chunking into buckets or using a pairing heap—`std.PriorityQueue` is binary and may incur cache misses for million-item heaps.
* When exposing queue factories across module boundaries, document allocator ownership and provide explicit `destroy` helpers to prevent leaks when callers change policies at runtime. xref:19__modules-and-imports-root-builtin-discovery.adoc[19]
