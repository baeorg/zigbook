////
changes: ["Added spawn/join workload example", "Documented atomic once pattern"]
examples_compile: yes
keywords: ["threads", "atomics", "@cmpxchg"]
last_updated: 2025-11-05
last_verified: 2025-11-05
next_chapter: "30__project-parallel-wordcount"
open_questions: []
previous_chapter: "28__filesystem-and-io"
status: draft
xref_complete: true
////

= Threads & Atomics
:chapter-number: 29
:chapter-slug: threads-and-atomics
:copyright: zigbook
:doctype: book
:embedded:
:experimental:
:icons: font
:partnums:
:pygments-linenums-mode: inline
:pygments-style: manni
:safe-mode-level: 0
:sectanchors:
:sectids:
:sectlinks:
:source-highlighter: pygments
:sourcedir: example$chapters-data/code
:webfonts:
:xrefstyle: short
:zig-version: 0.15.2
:linkcss:
:stylesdir: styles
:stylesheet: zigbook.css

[[overview]]
== Overview

Filesystem plumbing from the previous chapter set the stage for applications that produce and consume data in parallel. Now we focus on how Zig launches OS threads, coordinates work across cores, and keeps shared state consistent with atomic operations (see xref:28__filesystem-and-io.adoc[28] and link:https://github.com/ziglang/zig/tree/master/lib/std/Thread.zig[Thread.zig]).

Zig 0.15.2’s thread primitives combine lightweight spawn APIs with explicit memory ordering, so you decide when a store becomes visible and when contention should block. Understanding these tools now will make the upcoming parallel wordcount project far less mysterious (see link:https://github.com/ziglang/zig/tree/master/lib/std/atomic.zig[atomic.zig] and xref:30__project-parallel-wordcount.adoc[30]).

[[learning-goals]]
== Learning Goals

* Spawn and join worker threads responsibly, selecting stack sizes and allocators only when necessary.
* Choose memory orderings for atomic loads, stores, and compare-and-swap loops when protecting shared state.
* Detect single-threaded builds at compile time and fall back to synchronous execution paths.


[[thread-model]]
== Orchestrating Work with `std.Thread`

Zig models kernel threads through `std.Thread`, exposing helpers to query CPU counts, configure stack sizes, and join handles deterministically. Unlike async I/O, these are real kernel threads—every spawn consumes OS resources, so batching units of work matters.

=== Thread Pool Pattern

Before diving into manual thread spawning, it's valuable to understand the thread pool pattern that Zig's own compiler uses for parallel work. The following diagram shows how `std.Thread.Pool` distributes work across workers:

[mermaid]
....
graph TB
    ThreadPool["ThreadPool<br/>(std.Thread.Pool)"]

    AstGen1["AstGen<br/>(File 1)"]
    AstGen2["AstGen<br/>(File 2)"]
    AstGen3["AstGen<br/>(File 3)"]

    Sema1["Sema<br/>(Function 1)"]
    Sema2["Sema<br/>(Function 2)"]

    Codegen1["CodeGen<br/>(Function 1)"]
    Codegen2["CodeGen<br/>(Function 2)"]

    ThreadPool --> AstGen1
    ThreadPool --> AstGen2
    ThreadPool --> AstGen3
    ThreadPool --> Sema1
    ThreadPool --> Sema2
    ThreadPool --> Codegen1
    ThreadPool --> Codegen2

    ZcuPerThread["Zcu.PerThread<br/>(per-thread state)"]

    Sema1 -.->|"uses"| ZcuPerThread
    Sema2 -.->|"uses"| ZcuPerThread
....

A thread pool maintains a fixed number of worker threads that pull work items from a queue, avoiding the overhead of repeatedly spawning and joining threads. The Zig compiler uses this pattern extensively: `std.Thread.Pool` dispatches AST generation, semantic analysis, and code generation tasks to workers. Each worker has per-thread state (`Zcu.PerThread`) to minimize synchronization—only the final results need mutex protection when merging into shared data structures like `InternPool.shards`. This architecture demonstrates key concurrent design principles: work units should be independent, shared state should be sharded or protected by mutexes, and per-thread caches reduce contention. When your workload involves many small tasks, prefer `std.Thread.Pool` over manual spawning; when you need a few long-running workers with specific responsibilities, manual `spawn`/`join` is appropriate.

[[thread-model-chunk]]
=== Chunking data with spawn/join

The example below partitions an array of integers across a dynamic number of workers, using an atomic fetch-add to accumulate the total of even numbers without locks. It adapts to the host CPU count but never spawns more threads than there are elements to process.

[source,zig]
----
include::{sourcedir}/29__threads-and-atomics/01_parallel_even_sum.zig[]
----


.Run
[source,shell]
----
$ zig run 01_parallel_even_sum.zig
----

.Output
[source,shell]
----
spawned 8 worker(s)
even sum (threads): 7264
even sum (sequential check): 7264
----

TIP: `std.atomic.Value` wraps plain integers and routes every access through `@atomicLoad`, `@atomicStore`, or `@atomicRmw`, shielding you from accidentally mixing atomic and non-atomic access to the same memory location.

[[thread-model-config]]
=== Spawn configuration and scheduling hints

`std.Thread.SpawnConfig` lets you override stack sizes or supply a custom allocator if the defaults are unsuitable (for example, deep recursion or pre-allocated arenas). Catch `Thread.getCpuCount()` errors to provide a safe fallback, and remember to use `Thread.yield()` or `Thread.sleep()` when you need cooperative scheduling while waiting on other threads to progress.

[[atomics]]
== Atomic state machines

Zig exposes LLVM’s atomic intrinsics directly: you pick an order such as `.acquire`, `.release`, or `.seq_cst`, and the compiler emits the matching fences. That clarity is valuable when you design small state machines—like a one-time initializer—that multiple threads must observe consistently.

[[atomics-once]]
=== Implementing a once guard with atomic builtins

This program builds a lock-free "call once" helper around `@cmpxchgStrong`. Threads spin only while another thread is running the initializer, then read the published value via an acquire load.

[source,zig]
----
include::{sourcedir}/29__threads-and-atomics/02_atomic_once.zig[]
----


.Run
[source,shell]
----
$ zig run 02_atomic_once.zig
----

.Output
[source,shell]
----
thread 0 observed 9157
thread 1 observed 9157
thread 2 observed 9157
thread 3 observed 9157
init calls: 1
config value: 9157
----

NOTE: `@cmpxchgStrong` returns `null` on success, so looping while it yields a value is a concise way to retry the CAS without allocating a mutex. Pair the final `@atomicStore` with `.release` to publish the results before any waiter performs its `.acquire` load.

[[single-threaded]]
== Single-threaded builds & fallbacks

Passing `-Dsingle-threaded=true` forces the compiler to reject any attempt to spawn OS threads. Code that might run in both configurations should branch on `builtin.single_threaded` at compile time and substitute an inline execution path. See link:https://github.com/ziglang/zig/tree/master/lib/std/builtin.zig[builtin.zig].

=== Understanding the Single-Threaded Flag

The `single_threaded` flag is part of the compiler's feature configuration system, affecting code generation and optimization:

[mermaid]
....
graph TB
    subgraph "Code Generation Features"
        Features["Feature Flags"]

        Features --> UnwindTables["unwind_tables: bool"]
        Features --> StackProtector["stack_protector: bool"]
        Features --> StackCheck["stack_check: bool"]
        Features --> RedZone["red_zone: ?bool"]
        Features --> OmitFramePointer["omit_frame_pointer: bool"]
        Features --> Valgrind["valgrind: bool"]
        Features --> SingleThreaded["single_threaded: bool"]

        UnwindTables --> EHFrame["Generate .eh_frame<br/>for exception handling"]

        StackProtector --> CanaryCheck["Stack canary checks<br/>buffer overflow detection"]

        StackCheck --> ProbeStack["Stack probing<br/>prevents overflow"]

        RedZone --> RedZoneSpace["Red zone optimization<br/>(x86_64, AArch64)"]

        OmitFramePointer --> NoFP["Omit frame pointer<br/>for performance"]

        Valgrind --> ValgrindSupport["Valgrind client requests<br/>for memory debugging"]

        SingleThreaded --> NoThreading["Assume single-threaded<br/>enable optimizations"]
    end
....

When `single_threaded` is true, the compiler assumes no concurrent access to memory, enabling several optimizations: atomic operations can be lowered to plain loads and stores (eliminating fence instructions), thread-local storage becomes regular globals, and synchronization primitives can be elided entirely. This flag is set via `-Dsingle-threaded=true` at build time and flows through `Compilation.Config` into code generation. Importantly, this is not just an API restriction—it fundamentally changes the generated code. Atomics compiled in single-threaded mode have weaker guarantees than atomics in multi-threaded builds, so you must ensure your code paths remain consistent across both modes to avoid subtle bugs when toggling the flag.

[[single-threaded-guard]]
=== Gating thread usage at compile time

The guard below resets an atomic state machine, then either spawns a worker or executes the task inline based on the build mode. Because the branch is compile-time, the single-threaded configuration never instantiates `Thread.spawn`, avoiding a compile error altogether.

[source,zig]
----
include::{sourcedir}/29__threads-and-atomics/03_single_thread_guard.zig[]
----


.Run
[source,shell]
----
$ zig run 03_single_thread_guard.zig
----

.Output
[source,shell]
----
multi-threaded build; spawning worker
task state: threaded_done
----

IMPORTANT: When you build with `-Dsingle-threaded=true`, the inline branch is the only one compiled, so keep the logic symmetrical and make sure any shared state is still set via the same atomic helpers to avoid diverging semantics.

[[notes-caveats]]
== Notes & Caveats

* Threads must be joined or detached exactly once; leaking handles leads to resource exhaustion. `Thread.join` consumes the handle, so store it in a slice you can iterate later.
* Atomics operate on raw memory—never mix atomic and non-atomic accesses to the same location, even if you 'know' the race cannot happen. Wrap shared scalars in `std.atomic.Value` to keep your intent obvious.
* Compare-and-swap loops may live-spin; consider `Thread.yield()` or event primitives like `Thread.ResetEvent` when a wait might last longer than a few cycles.

=== Debugging Concurrent Code with ThreadSanitizer

Zig provides built-in race detection through ThreadSanitizer, a powerful tool for finding data races, deadlocks, and other concurrency bugs:

|===
| Sanitizer | Config Field | Purpose | Requirements

| Thread Sanitizer
| `any_sanitize_thread`
| Data race detection
| LLVM backend

| UBSan
| `any_sanitize_c`
| C undefined behavior
| LLVM backend, C code

| Fuzzing
| `any_fuzz`
| Fuzzing instrumentation
| libfuzzer integration
|===

[mermaid]
....
graph TB
    subgraph "Sanitizer Configuration"
        Sanitizers["Sanitizer Flags"]

        Sanitizers --> TSan["any_sanitize_thread"]
        Sanitizers --> UBSan["any_sanitize_c"]
        Sanitizers --> Fuzz["any_fuzz"]

        TSan --> TSanLib["tsan_lib: ?CrtFile"]
        TSan --> TSanRuntime["ThreadSanitizer runtime<br/>linked into binary"]

        UBSan --> UBSanLib["ubsan_rt_lib: ?CrtFile<br/>ubsan_rt_obj: ?CrtFile"]
        UBSan --> UBSanRuntime["UBSan runtime<br/>C undefined behavior checks"]

        Fuzz --> FuzzLib["fuzzer_lib: ?CrtFile"]
        Fuzz --> FuzzRuntime["libFuzzer integration<br/>for fuzz testing"]
    end
....

Enable ThreadSanitizer with `-Dsanitize-thread` when building your program. TSan instruments all memory accesses and synchronization operations, tracking happens-before relationships to detect races. When a race is detected, TSan prints detailed reports showing the conflicting accesses and their stack traces. The instrumentation adds significant runtime overhead (2-5x slowdown, 5-10x memory usage), so use it during development and testing, not in production. TSan is particularly valuable for validating atomic code: even if your logic appears correct, TSan can catch subtle ordering issues or missing synchronization. For the examples in this chapter, try running them with `-Dsanitize-thread` to verify they're race-free—the parallel sum and atomic once patterns should pass cleanly, demonstrating proper synchronization.

[[exercises]]
== Exercises

* Extend the parallel sum to accept a predicate callback so you can swap "even numbers" for any classification you like; measure the effect of `.acquire` vs `.monotonic` loads on contention.
* Rework the `callOnce` demo to stage errors: have the initializer return `!void` and store the failure in an atomic slot so callers can rethrow the same error consistently.
* Introduce a `std.Thread.WaitGroup` around the once-guard code so you can wait for arbitrary numbers of worker threads without storing handles manually.

[[caveats-alternatives-edge-cases]]
== Caveats, alternatives, edge cases

* On platforms without pthreads or Win32 threads Zig emits a compile error; plan to fall back to event loops or async when targeting WASI without `--threading` support.
* Atomics operate on plain integers and enums; for composite state consider using a mutex or designing an array of atomics to avoid torn updates.
* Single-threaded builds can still use atomics, but the instructions compile to ordinary loads/stores. Keep the code paths consistent so you do not rely accidentally on the stronger ordering in multi-threaded builds.

=== Platform-Specific Threading Constraints

Not all platforms support threading, and some have special requirements for thread-local storage:

[mermaid]
....
graph TB
    subgraph "Threading Configuration"
        TARG["Target Platform"]
        TARG --> SINGLETHREAD["defaultSingleThreaded()<br/>WASM, Haiku"]
        TARG --> EMULATETLS["useEmulatedTls()<br/>OpenBSD, old Android"]

        SINGLETHREAD --> NOTHREAD["No thread support"]
        EMULATETLS --> TLSEMU["Software TLS"]
    end
....

Certain targets default to single-threaded mode because they lack OS thread support: WebAssembly (without the `--threading` flag) and Haiku OS both fall into this category. On these platforms, attempting to spawn threads results in a compile error unless you've explicitly enabled threading support in your build configuration. A related concern is thread-local storage (TLS): OpenBSD and older Android versions don't provide native TLS, so Zig uses emulated TLS—a software implementation that's slower but portable. When writing cross-platform concurrent code, check `target.defaultSingleThreaded()` and `target.useEmulatedTls()` to understand platform constraints. For WASM, you can enable threading with the `atomics` and `bulk-memory` features plus the `--import-memory --shared-memory` linker flags, but not all WASM runtimes support this. Design your code to gracefully degrade: use `builtin.single_threaded` to provide synchronous fallbacks, and avoid assuming TLS is zero-cost on all platforms.

[[summary]]
== Summary

* `std.Thread` offers lightweight spawn/join semantics, but you remain responsible for scheduling and cleanup.
* Atomic intrinsics such as `@atomicLoad`, `@atomicStore`, and `@cmpxchgStrong` make small lock-free state machines practical when you match the orderings to your invariant.
* Using `builtin.single_threaded` keeps shared components working across single-threaded builds and multi-core deployments without forking the codebase.
