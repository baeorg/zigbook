////
changes: ["Detailed workspace orchestration techniques", "Target matrix pipelines using std.Build hooks"]
examples_compile: yes
keywords: ["std.Build", "workspace", "multi-target", "named write files", "dependency overrides"]
last_updated: 2025-11-05
last_verified: 2025-11-05
next_chapter: "27__project-multi-package-workspace-and-vendor"
open_questions: []
previous_chapter: "25__module-resolution-and-discovery-deep"
status: draft
xref_complete: true
////

= Build System Advanced Topics
:chapter-number: 26
:chapter-slug: build-system-advanced-topics
:copyright: zigbook
:doctype: book
:embedded:
:experimental:
:icons: font
:partnums:
:pygments-linenums-mode: inline
:pygments-style: manni
:safe-mode-level: 0
:sectanchors:
:sectids:
:sectlinks:
:source-highlighter: pygments
:sourcedir: example$chapters-data/code
:webfonts:
:xrefstyle: short
:zig-version: 0.15.2
:linkcss:
:stylesdir: styles
:stylesheet: zigbook.css

[[overview]]
== Overview

Module resolution gave us the vocabulary for reasoning about the compiler's graph. Now we turn that vocabulary into infrastructure. This chapter digs into `std.Build` beyond the basics, exploring artifact tours and library/executable workspaces. We will register modules intentionally, compose multi-package workspaces, generate build outputs without touching shell scripts, and drive cross-target matrices from a single `build.zig`. See link:https://github.com/ziglang/zig/tree/master/lib/std/Build.zig[Build.zig].

You will learn how named write-files, anonymous modules, and `resolveTargetQuery` feed the build runner, how to keep vendored code isolated from registry dependencies, and how to wire CI jobs that prove your graph behaves in Debug and Release builds alike. See link:https://github.com/ziglang/zig/blob/master/lib/compiler/build_runner.zig[build_runner.zig].

=== How the Build System Executes

Before diving into advanced patterns, it's essential to understand how `std.Build` executes. The following diagram shows the complete flow from the Zig compiler invoking your `build.zig` script through to final artifact installation:

[mermaid]
....
graph TB
    subgraph "CMake Stage (stage2)"
        CMAKE["CMake"]
        ZIG2_C["zig2.c<br/>(generated C code)"]
        ZIGCPP["zigcpp<br/>(C++ LLVM/Clang wrapper)"]
        ZIG2["zig2 executable"]

        CMAKE --> ZIG2_C
        CMAKE --> ZIGCPP
        ZIG2_C --> ZIG2
        ZIGCPP --> ZIG2
    end

    subgraph "Native Build System (stage3)"
        BUILD_ZIG["build.zig<br/>Native Build Script"]
        BUILD_FN["build() function"]
        COMPILER_STEP["addCompilerStep()"]
        EXE["std.Build.Step.Compile<br/>(compiler executable)"]
        INSTALL["Installation Steps"]

        BUILD_ZIG --> BUILD_FN
        BUILD_FN --> COMPILER_STEP
        COMPILER_STEP --> EXE
        EXE --> INSTALL
    end

    subgraph "Build Arguments"
        ZIG_BUILD_ARGS["ZIG_BUILD_ARGS<br/>--zig-lib-dir<br/>-Dversion-string<br/>-Dtarget<br/>-Denable-llvm<br/>-Doptimize"]
    end

    ZIG2 -->|"zig2 build"| BUILD_ZIG
    ZIG_BUILD_ARGS --> BUILD_FN

    subgraph "Output"
        STAGE3_BIN["stage3/bin/zig"]
        STD_LIB["stage3/lib/zig/std/"]
        LANGREF["stage3/doc/langref.html"]
    end

    INSTALL --> STAGE3_BIN
    INSTALL --> STD_LIB
    INSTALL --> LANGREF
....

Your `build.zig` is a regular Zig program compiled and executed by the compiler. The `build()` function is the entry point, receiving a `*std.Build` instance that provides the API for defining steps, artifacts, and dependencies. Build arguments (`-D` flags) are parsed by `b.option()` and flow into your build logic as compile-time constants. The build runner then traverses the step dependency graph you've declared, executing only the steps needed to satisfy the requested target (defaulting to the install step). This declarative model ensures reproducibility: the same inputs always produce the same build graph.

[[learning-goals]]
== Learning Goals

* Register reusable modules and anonymous packages explicitly, controlling which names appear in the import namespace. xref:25__module-resolution-and-discovery-deep.adoc[25]
* Generate deterministic artifacts (reports, manifests) from the build graph using named write-files instead of ad-hoc shell scripting.
* Coordinate multi-target builds with `resolveTargetQuery`, including host sanity checks and cross-compilation pipelines. xref:22__build-system-deep-dive.adoc[22], link:https://github.com/ziglang/zig/tree/master/lib/std/Build/Step/Compile.zig[Compile.zig]
* Structure composite workspaces so vendored modules remain private while registry packages stay self-contained. xref:24__zig-package-manager-deep.adoc[24]
* Capture reproducibility guarantees in CI: install steps, run steps, and generated artifacts all hang off `std.Build.Step` dependencies.

[[workspace-surface]]
== Building a Workspace Surface

A workspace is just a build graph with clear namespace boundaries. The following example promotes three modules—`analytics`, `reporting`, and a vendored `adapters` helper—and shows how a root executable consumes them. We emphasize which modules are globally registered, which remain anonymous, and how to emit documentation straight from the build graph.

[source,zig]
----
include::{sourcedir}/26__build-system-advanced-topics/01_workspace_build.zig[]
----

The `build()` function follows a deliberate cadence:

* `b.addModule("analytics", …)` registers a public name so the entire workspace can `@import("analytics")`. link:https://github.com/ziglang/zig/tree/master/lib/std/Build/Module.zig[Module.zig]
* `b.createModule` creates a private module (`adapters`) that only the root executable sees—ideal for vendored code that consumers should not reach. xref:24__zig-package-manager-deep.adoc[24]
* `b.addNamedWriteFiles("workspace-graph")` produces a `module-graph.txt` file in `zig-out/`, documenting the namespace mapping without bespoke tooling.
* Every dependency is threaded through `.imports`, so the compiler never falls back to filesystem guessing. xref:25__module-resolution-and-discovery-deep.adoc[25]

.Run workspace app
[source,shell]
----
$ zig build --build-file 01_workspace_build.zig run
----

.Output
[source,shell]
----
metric: response_ms
count: 6
mean: 12.95
deviation: 1.82
profile: stable
json export: {
  "name": "response_ms",
  "mean": 12.950,
  "deviation": 1.819,
  "profile": "stable"
}
----

.Generate module graph
[source,shell]
----
$ zig build --build-file 01_workspace_build.zig graph
----

.Output
[source,shell]
----
No stdout expected.
----

NOTE: Named write-files obey the cache: rerunning `zig build … graph` without changes is instant. Check `zig-out/graph/module-graph.txt` to see the mapping emitted by the build runner.

[[workspace-library-code]]
=== Library code for the workspace

To keep this example self-contained, the modules live next to the build script. Feel free to adapt them to your needs or swap in registry dependencies declared in `build.zig.zon`.

[source,zig]
----
include::{sourcedir}/26__build-system-advanced-topics/workspace/analytics/lib.zig[]
----

[source,zig]
----
include::{sourcedir}/26__build-system-advanced-topics/workspace/reporting/lib.zig[]
----

[source,zig]
----
include::{sourcedir}/26__build-system-advanced-topics/workspace/adapters/vendored.zig[]
----

[source,zig]
----
include::{sourcedir}/26__build-system-advanced-topics/workspace/app/main.zig[]
----

TIP: `std.fmt.allocPrint` pairs well with allocator plumbing when you want build-time helpers to operate without heap globals. Prefer it over ad-hoc `ArrayList` usage when emitting CSV or JSON snapshots in Zig 0.15.2. See link:https://ziglang.org/download/0.15.1/release-notes.html#upgrading-stdiogetstdoutwriterprint[v0.15.2] and link:https://github.com/ziglang/zig/tree/master/lib/std/fmt.zig[fmt.zig].

[[dependency-hygiene]]
=== Dependency hygiene checklist

* Register vendored modules with distinct names and share them only via `.imports`. Do **not** leak them through `b.addModule` unless consumers are expected to import them directly.
* Treat `zig-out/workspace-graph/module-graph.txt` as living documentation. Commit outputs for CI verification or diff them to catch accidental namespace changes.
* For registry dependencies, forward `b.dependency()` handles exactly once and wrap them in local modules. This keeps upgrade churn isolated. xref:24__zig-package-manager-deep.adoc[24]

=== Build Options as Configuration

Build options provide a powerful mechanism for making your workspace configurable. The following diagram shows how command-line `-D` flags flow through `b.option()`, get added to a generated module via `b.addOptions()`, and become compile-time constants accessible via `@import("build_options")`:

[mermaid]
....
graph LR
    subgraph "Command Line"
        CLI["-Ddebug-allocator<br/>-Denable-llvm<br/>-Dversion-string<br/>etc."]
    end

    subgraph "build.zig"
        PARSE["b.option()<br/>Parse options"]
        OPTIONS["exe_options =<br/>b.addOptions()"]
        ADD["exe_options.addOption()"]

        PARSE --> OPTIONS
        OPTIONS --> ADD
    end

    subgraph "Generated Module"
        BUILD_OPTIONS["build_options<br/>(auto-generated)"]
        CONSTANTS["pub const mem_leak_frames = 4;<br/>pub const have_llvm = true;<br/>pub const version = '0.16.0';<br/>etc."]

        BUILD_OPTIONS --> CONSTANTS
    end

    subgraph "Compiler Source"
        IMPORT["@import('build_options')"]
        USE["if (build_options.have_llvm) { ... }"]

        IMPORT --> USE
    end

    CLI --> PARSE
    ADD --> BUILD_OPTIONS
    BUILD_OPTIONS --> IMPORT
....

This pattern is essential for parameterized workspaces. Use `b.option(bool, "feature-x", "Enable feature X")` to declare options, then call `options.addOption("feature_x", feature_x)` to make them available at compile time. The generated module is automatically rebuilt when options change, ensuring your binaries always reflect the current configuration. This technique works for version strings, feature flags, debug settings, and any other build-time constant your code needs.

[[target-matrix]]
== Target Matrices and Release Channels

Complex projects often ship multiple binaries: debug utilities for contributors, ReleaseFast builds for production, and WASI artifacts for automation. Rather than duplicating build logic per target, assemble a matrix that iterates over `std.Target.Query` definitions.

=== Understanding Target Resolution

Before iterating over targets, it's important to understand how `b.resolveTargetQuery` transforms partial specifications into fully-resolved targets. The following diagram shows the resolution process:

[mermaid]
....
graph LR
    subgraph "User Input"
        Query["Target.Query"]
        Query --> QCpu["cpu_arch: ?Cpu.Arch"]
        Query --> QModel["cpu_model: CpuModel"]
        Query --> QOs["os_tag: ?Os.Tag"]
        Query --> QAbi["abi: ?Abi"]
    end

    subgraph "Resolution Process"
        Resolve["resolveTargetQuery()"]
        Query --> Resolve
        Detection["Native Detection"]
        Defaults["Apply Defaults"]
        Detection --> Resolve
        Defaults --> Resolve
    end

    subgraph "Fully Resolved"
        Target["Target"]
        Resolve --> Target
        Target --> TCpu["cpu: Cpu"]
        Target --> TOs["os: Os"]
        Target --> TAbi["abi: Abi"]
        Target --> TOfmt["ofmt: ObjectFormat"]
    end
....

When you pass a `Target.Query` with `null` CPU or OS fields, the resolver detects your native platform and fills in concrete values. Similarly, if you specify an OS without an ABI, the resolver applies the default ABI for that OS (e.g., `.gnu` for Linux, `.msvc` for Windows). This resolution happens once per query and produces a `ResolvedTarget` containing the fully-specified `Target` plus metadata about whether values came from native detection. Understanding this distinction is crucial for cross-compilation: a query with `.cpu_arch = .x86_64` and `.os_tag = .linux` yields a different resolved target on each host platform due to CPU model and feature detection.

[source,zig]
----
include::{sourcedir}/26__build-system-advanced-topics/02_multi_target_matrix.zig[]
----

Key techniques:

* Predeclare a slice of `{ name, query, optimize }` combos. Queries match `zig build -Dtarget` semantics but stay type-checked.
* `b.resolveTargetQuery` converts each query into a `ResolvedTarget` so the module inherits canonical CPU/OS defaults.
* Aggregating everything under a `matrix` step keeps CI wiring clean: call `zig build -Drelease-mode=fast matrix` (or leave defaults) and let dependencies ensure artefacts exist.
* Running the first (host) target as part of the matrix catches regressions without cross-runner emulation. For deeper coverage, enable `b.enable_qemu` / `b.enable_wasmtime` before calling `addRunArtifact`.

.Run matrix build
[source,shell]
----
$ zig build --build-file 02_multi_target_matrix.zig matrix
----

.Output (host variant)
[listing]
----
target: x86_64-linux-gnu
optimize: Debug
----

=== Running Cross-Compiled Targets

When your matrix includes cross-compilation targets, you'll need external executors to actually run the binaries. The build system automatically selects the appropriate executor based on host/target compatibility:

[mermaid]
....
flowchart TD
    Start["getExternalExecutor(host, candidate)"]

    CheckMatch{"OS + CPU\ncompatible?"}
    CheckDL{"link_libc &&\nhas dynamic_linker?"}
    DLExists{"Dynamic linker\nexists on host?"}

    Native["Executor.native"]

    CheckRosetta{"macOS + arm64 host\n&& x86_64 target?"}
    Rosetta["Executor.rosetta"]

    CheckQEMU{"OS matches &&\nallow_qemu?"}
    QEMU["Executor.qemu\n(e.g., 'qemu-aarch64')"]

    CheckWasmtime{"target.isWasm() &&\nallow_wasmtime?"}
    Wasmtime["Executor.wasmtime"]

    CheckWine{"target.os == .windows\n&& allow_wine?"}
    Wine["Executor.wine"]

    CheckDarling{"target.os.isDarwin()\n&& allow_darling?"}
    Darling["Executor.darling"]

    BadDL["Executor.bad_dl"]
    BadOsCpu["Executor.bad_os_or_cpu"]

    Start --> CheckMatch
    CheckMatch --> |Yes|CheckDL
    CheckMatch --> |No|CheckRosetta

    CheckDL --> |No libc|Native
    CheckDL --> |Has libc|DLExists
    DLExists --> |Yes|Native
    DLExists --> |No|BadDL

    CheckRosetta --> |Yes|Rosetta
    CheckRosetta --> |No|CheckQEMU

    CheckQEMU --> |Yes|QEMU
    CheckQEMU --> |No|CheckWasmtime

    CheckWasmtime --> |Yes|Wasmtime
    CheckWasmtime --> |No|CheckWine

    CheckWine --> |Yes|Wine
    CheckWine --> |No|CheckDarling

    CheckDarling --> |Yes|Darling
    CheckDarling --> |No|BadOsCpu
....

Enable emulators in your build script by setting `b.enable_qemu = true` or `b.enable_wasmtime = true` before calling `addRunArtifact`. On macOS ARM hosts, x86_64 targets automatically use Rosetta 2. For Linux cross-architecture testing, QEMU user-mode emulation runs ARM/RISC-V/MIPS binaries transparently when the OS matches. WASI targets require Wasmtime, while Windows binaries on Linux can use Wine. If no executor is available, the run step will fail with `Executor.bad_os_or_cpu`—detect this early by testing matrix coverage on representative CI hosts.

CAUTION: Cross targets that rely on native system libraries (e.g. glibc) need appropriate sysroot packs. Populate `ZIG_LIBC` or configure `b.libc_file` before adding those combos to production pipelines.

[[vendoring-vs-registry]]
== Vendoring vs Registry Dependencies

* **Registry-first approach**: keep `build.zig.zon` hashes authoritative, then register each dependency module via `b.dependency()` and `module.addImport()`. xref:24__zig-package-manager-deep.adoc[24]
* **Vendor-first approach**: drop sources into `deps/<name>/` and wire them with `b.addAnonymousModule` or `b.createModule`. Document the provenance in `module-graph.txt` so collaborators know which code is pinned locally.
* Whichever strategy you choose, record a policy in CI: a step that fails if `zig out/workspace-graph/module-graph.txt` changes unexpectedly, or a lint that checks vendored directories for LICENSE files.

[[ci-scenarios]]
== CI Scenarios and Automation Hooks

=== Step Dependencies in Practice

CI pipelines benefit from understanding how build steps compose. The following diagram shows a real-world step dependency graph from the Zig compiler's own build system:

[mermaid]
....
graph TB
    subgraph "Installation Step (default)"
        INSTALL["b.getInstallStep()"]
    end

    subgraph "Compiler Artifacts"
        EXE_STEP["exe.step<br/>(compile compiler)"]
        INSTALL_EXE["install_exe.step<br/>(install binary)"]
    end

    subgraph "Documentation"
        LANGREF["generateLangRef()"]
        INSTALL_LANGREF["install_langref.step"]
        STD_DOCS_GEN["autodoc_test"]
        INSTALL_STD_DOCS["install_std_docs.step"]
    end

    subgraph "Library Files"
        LIB_FILES["installDirectory(lib/)"]
    end

    subgraph "Test Steps"
        TEST["test step"]
        FMT["test-fmt step"]
        CASES["test-cases step"]
        MODULES["test-modules step"]
    end

    INSTALL --> INSTALL_EXE
    INSTALL --> INSTALL_LANGREF
    INSTALL --> LIB_FILES

    INSTALL_EXE --> EXE_STEP
    INSTALL_LANGREF --> LANGREF
    INSTALL --> INSTALL_STD_DOCS
    INSTALL_STD_DOCS --> STD_DOCS_GEN

    TEST --> EXE_STEP
    TEST --> FMT
    TEST --> CASES
    TEST --> MODULES

    CASES --> EXE_STEP
    MODULES --> EXE_STEP
....

Notice how the default install step (`zig build`) depends on binary installation, documentation, and library files—but **not** tests. Meanwhile, the test step depends on compilation plus all test substeps. This separation lets CI run `zig build` for release artifacts and `zig build test` for validation in parallel jobs. Each step only executes when its dependencies change, thanks to content-addressed caching. You can inspect this graph locally with `zig build --verbose` or by adding a custom step that dumps dependencies.

=== Automation Patterns

* **Artifact verification**: Add a `zig build graph` job that uploads `module-graph.txt` alongside compiled binaries. Consumers can diff namespaces between releases.
* **Matrix extension**: Parameterize the combos array via build options (`-Dinclude-windows=true`). Use `b.option(bool, "include-windows", …)` to let CI toggle extra targets without editing source.
* **Security posture**: Pipe `zig build --fetch` (Chapter 24) into the matrix run so caches populate before cross jobs run offline. See xref:24__zig-package-manager-deep.adoc[24].
* **Reproducibility**: Teach CI to run `zig build install` twice and assert no files change between runs. Because `std.Build` respects content hashing, the second invocation should no-op unless inputs changed.

=== Advanced Test Organization

For comprehensive projects, organizing tests into categories with matrix application requires careful step composition. The following diagram shows a production-grade test hierarchy:

[mermaid]
....
graph TB
    subgraph "Test Steps"
        TEST_STEP["test step<br/>(umbrella step)"]
        FMT["test-fmt<br/>Format checking"]
        CASES["test-cases<br/>Compiler test cases"]
        MODULES["test-modules<br/>Per-target module tests"]
        UNIT["test-unit<br/>Compiler unit tests"]
        STANDALONE["Standalone tests"]
        CLI["CLI tests"]
        STACK_TRACE["Stack trace tests"]
        ERROR_TRACE["Error trace tests"]
        LINK["Link tests"]
        C_ABI["C ABI tests"]
        INCREMENTAL["test-incremental<br/>Incremental compilation"]
    end

    subgraph "Module Tests"
        BEHAVIOR["behavior tests<br/>test/behavior.zig"]
        COMPILER_RT["compiler_rt tests<br/>lib/compiler_rt.zig"]
        ZIGC["zigc tests<br/>lib/c.zig"]
        STD["std tests<br/>lib/std/std.zig"]
        LIBC_TESTS["libc tests"]
    end

    subgraph "Test Configuration"
        TARGET_MATRIX["test_targets array<br/>Different architectures<br/>Different OSes<br/>Different ABIs"]
        OPT_MODES["Optimization modes:<br/>Debug, ReleaseFast<br/>ReleaseSafe, ReleaseSmall"]
        FILTERS["test-filter<br/>test-target-filter"]
    end

    TEST_STEP --> FMT
    TEST_STEP --> CASES
    TEST_STEP --> MODULES
    TEST_STEP --> UNIT
    TEST_STEP --> STANDALONE
    TEST_STEP --> CLI
    TEST_STEP --> STACK_TRACE
    TEST_STEP --> ERROR_TRACE
    TEST_STEP --> LINK
    TEST_STEP --> C_ABI
    TEST_STEP --> INCREMENTAL

    MODULES --> BEHAVIOR
    MODULES --> COMPILER_RT
    MODULES --> ZIGC
    MODULES --> STD

    TARGET_MATRIX --> MODULES
    OPT_MODES --> MODULES
    FILTERS --> MODULES
....

The umbrella test step aggregates all test categories, letting you run the full suite with `zig build test`. Individual categories can be invoked separately (`zig build test-fmt`, `zig build test-modules`) for faster iteration. Notice how only the module tests receive matrix configuration—format checking and CLI tests don't vary by target. Use `b.option([]const u8, "test-filter", …)` to let CI run subsets, and apply optimization modes selectively based on test type. This pattern scales to hundreds of test files while keeping build times manageable through parallel execution and caching.

[[notes-caveats]]
== Notes & Caveats

* `b.addModule` registers a name globally for the current build graph; `b.createModule` keeps the module private. Mixing them up leads to surprising imports or missing symbols. xref:25__module-resolution-and-discovery-deep.adoc[25]
* Named write-files respect the cache. Delete `.zig-cache` if you need to regenerate them from scratch; otherwise the step can trick you into thinking a change landed when it actually hit the cache.
* When iterating matrices, always prune stale binaries with `zig build uninstall` (or a custom `Step.RemoveDir`) to avoid cross-version confusion.

=== Under the Hood: Dependency Tracking

The build system's caching and incremental behavior relies on the compiler's sophisticated dependency tracking infrastructure. Understanding this helps explain why cached builds are so fast and why certain changes trigger broader rebuilds than expected.

[mermaid]
....
graph TB
    subgraph "InternPool - Dependency Storage"
        SRCHASHDEPS["src_hash_deps<br/>Map: TrackedInst.Index → DepEntry.Index"]
        NAVVALDEPS["nav_val_deps<br/>Map: Nav.Index → DepEntry.Index"]
        NAVTYDEPS["nav_ty_deps<br/>Map: Nav.Index → DepEntry.Index"]
        INTERNEDDEPS["interned_deps<br/>Map: Index → DepEntry.Index"]
        ZONFILEDEPS["zon_file_deps<br/>Map: FileIndex → DepEntry.Index"]
        EMBEDFILEDEPS["embed_file_deps<br/>Map: EmbedFile.Index → DepEntry.Index"]
        NSDEPS["namespace_deps<br/>Map: TrackedInst.Index → DepEntry.Index"]
        NSNAMEDEPS["namespace_name_deps<br/>Map: NamespaceNameKey → DepEntry.Index"]

        FIRSTDEP["first_dependency<br/>Map: AnalUnit → DepEntry.Index"]
        DEPENTRIES["dep_entries<br/>ArrayListUnmanaged<DepEntry>"]
        FREEDEP["free_dep_entries<br/>ArrayListUnmanaged<DepEntry.Index>"]
    end

    subgraph "DepEntry Structure"
        DEPENTRY["DepEntry<br/>{depender: AnalUnit,<br/>next_dependee: DepEntry.Index.Optional,<br/>next_depender: DepEntry.Index.Optional}"]
    end

    SRCHASHDEPS --> DEPENTRIES
    NAVVALDEPS --> DEPENTRIES
    NAVTYDEPS --> DEPENTRIES
    INTERNEDDEPS --> DEPENTRIES
    ZONFILEDEPS --> DEPENTRIES
    EMBEDFILEDEPS --> DEPENTRIES
    NSDEPS --> DEPENTRIES
    NSNAMEDEPS --> DEPENTRIES
    FIRSTDEP --> DEPENTRIES

    DEPENTRIES --> DEPENTRY
    FREEDEP -.->|"reuses indices from"| DEPENTRIES
....

The compiler tracks dependencies at multiple granularities: source file hashes (`src_hash_deps`), navigation values (`nav_val_deps`), types (`nav_ty_deps`), interned constants, ZON files, embedded files, and namespace membership. All these maps point into a shared `dep_entries` array containing `DepEntry` structures that form linked lists. Each entry participates in two lists: one linking all analysis units that depend on a particular dependee (traversed during invalidation), and one linking all dependees of a particular analysis unit (traversed during cleanup). When you modify a source file, the compiler hashes it, looks up dependents in `src_hash_deps`, and marks only those analysis units as outdated. This granular tracking is why changing a private function in one file doesn't rebuild unrelated modules—the dependency graph precisely captures what actually depends on what. The build system leverages this infrastructure through content addressing: step outputs are cached by their input hashes, and reused when inputs haven't changed.

[[exercises]]
== Exercises

* Extend `01_workspace_build.zig` so the `graph` step emits both a human-readable table and a JSON document. Hint: call `graph_files.add("module-graph.json", …)` with `std.json` output. See link:https://github.com/ziglang/zig/tree/master/lib/std/json.zig[json.zig].
* Add a `-Dtarget-filter` option to `02_multi_target_matrix.zig` that limits matrix execution to a comma-separated allowlist. Use `std.mem.splitScalar` to parse the value. xref:22__build-system-deep-dive.adoc[22]
* Introduce a registry dependency via `b.dependency("logging", .{})` and expose it to the workspace with `module.addImport("logging", dep.module("logging"))`. Document the new namespace in `module-graph.txt`.

[[caveats-alternatives-edge-cases]]
== Caveats, alternatives, edge cases

* Large workspaces might exceed default install directory limits. Use `b.setInstallPrefix` or `b.setLibDir` before adding artifacts to route outputs into per-target directories.
* On Windows, `resolveTargetQuery` requires `abi = .msvc` if you expect MSVC-compatible artifacts; the default `.gnu` ABI yields MinGW binaries.
* If you supply anonymous modules to dependencies, remember they are not deduplicated. Reuse the same `b.createModule` instance when multiple artefacts need the same vendored code.

[[summary]]
== Summary

* Workspaces stay predictable when you register every module explicitly and document the mapping via named write-files.
* `resolveTargetQuery` and iteration-friendly combos let you scale to multiple targets without copy/pasting build logic.
* CI jobs benefit from `std.Build` primitives: steps articulate dependencies, run artefacts gate sanity checks, and named artefacts capture reproducible metadata.

Together with Chapters 22–25, you now have the tools to craft deterministic Zig build graphs that scale across packages, targets, and release channels.
