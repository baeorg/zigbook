////
changes: ["Initial GPU compute project chapter"]
examples_compile: yes
keywords: ["gpu", "spirv", "compute"]
last_updated: 2025-11-06
last_verified: 2025-11-06
next_chapter: "36__style-and-best-practices"
open_questions: []
previous_chapter: "34__gpu-fundamentals"
status: draft
xref_complete: true
////

= Project: GPU Compute in Zig
:chapter-number: 35
:chapter-slug: project-gpu-compute-in-zig
:copyright: zigbook
:doctype: book
:embedded:
:experimental:
:icons: font
:partnums:
:pygments-linenums-mode: inline
:pygments-style: manni
:safe-mode-level: 0
:sectanchors:
:sectids:
:sectlinks:
:source-highlighter: pygments
:sourcedir: example$chapters-data/code
:webfonts:
:xrefstyle: short
:zig-version: 0.15.2
:linkcss:
:stylesdir: styles
:stylesheet: zigbook.css

[[overview]]
== Overview

xref:34__gpu-fundamentals.adoc[Chapter 34] outlined the GPU execution model, address spaces, and dispatch planning; now we build an end-to-end workload that starts with Zig source and ends with a validated binary dump ready for submission to Vulkan or WebGPU queue families. link:https://github.com/ziglang/zig/tree/master/lib/std/Target.zig[Target.zig] The project stitches together three pieces: a SPIR-V kernel authored in pure Zig, a host-side orchestration CLI with a CPU fallback, and a diff utility for comparing captured GPU buffers against expected results. link:https://github.com/ziglang/zig/tree/master/lib/std/build.zig[build.zig]

[[learning-goals]]
== Learning Goals

* Translate a Zig compute kernel into SPIR-V with the self-hosted backend and understand the resource layouts it expects.
* Coordinate buffers, dispatch geometry, and validation paths from a host application that can run with or without GPU access.
* Build lightweight diagnostics that evaluate GPU output against a deterministic CPU reference.

_Refs: xref:34__gpu-fundamentals.adoc[], link:https://github.com/ziglang/zig/tree/master/lib/std/Random.zig[Random.zig]_

[[pipeline-topology]]
== Building the Compute Pipeline

Our workload squares elements of a vector. The host creates submission metadata and data buffers, the kernel squares each lane, and the diff tool verifies device captures. The static lane capacity mirrors the GPU storage-buffer layout, while the host enforces logical bounds so the kernel can bail out when extra threads are scheduled. link:https://github.com/ziglang/zig/tree/master/lib/std/builtin.zig[builtin.zig]

[[pipeline-topology-flow]]
=== Topology and Data Flow

The dispatch is intentionally modest (1000 elements in blocks of 64 threads), so you can focus on correctness rather than occupancy tuning. The host injects random floating-point values, records a checksum for observability, and emits a binary blob that downstream tooling—or a real GPU driver—can reuse. Because storage buffers operate on raw bytes, we pair every pointer parameter with an `extern struct` facade to guarantee layout parity with descriptor tables.

[[authoring-kernel]]
== Authoring the SPIR-V Kernel

The kernel receives three storage buffers: a submission header describing the logical length, an input vector, and an output vector. Each invocation reads one lane, squares it, and writes the result back if it falls within bounds. Defensive checks prevent stray workgroups from touching memory past the logical length, a common hazard when the host pads the dispatch to match wavefront granularity.

[source,zig]
----
include::{sourcedir}/35__project-gpu-compute-in-zig/01_vector_square_kernel.zig[]
----


.Run
[source,shell]
----
$ zig build-obj -fno-llvm -O ReleaseSmall -target spirv32-vulkan-none \
    -femit-bin=kernels/vector_square.spv \
    01_vector_square_kernel.zig
----

.Output
[source,shell]
----
no output (binary module generated)
----

NOTE: Delete `kernels/vector_square.spv` when you finish experimenting so repeated runs always rebuild the shader from source. link:https://github.com/ziglang/zig/tree/master/lib/std/fs.zig[fs.zig]

[[host-orchestration]]
== Host Orchestration and CPU Fallback

The host CLI plans the dispatch, seeds deterministic input, runs a CPU fallback, and—when requested—writes a reference dump to `out/reference.bin`. It also validates the SPIR-V header (0x07230203) so broken builds surface immediately instead of failing deep inside a graphics API. Optional hooks let you drop in a captured GPU buffer (`out/gpu_result.bin`) for post-run comparison.

[source,zig]
----
include::{sourcedir}/35__project-gpu-compute-in-zig/02_host_pipeline.zig[]
----
link:https://github.com/ziglang/zig/tree/master/lib/std/math.zig[math.zig]

.Run
[source,shell]
----
$ zig build-obj -fno-llvm -O ReleaseSmall -target spirv32-vulkan-none \
    -femit-bin=kernels/vector_square.spv \
    01_vector_square_kernel.zig
$ zig run 02_host_pipeline.zig -- --emit-binary
----

.Output
[source,shell]
----
launch plan: 16 groups × 64 lanes => 1024 invocations (tail 24)
cpu fallback checksum: 83467485.758038
gpu module: kernels/vector_square.spv (5368 bytes, header ok)
gpu capture diff: skipped (no out/gpu_result.bin file found)
lane   0: in=0.10821 out=0.01171
lane   1: in=1.07972 out=1.16579
lane   2: in=1.03577 out=1.07281
lane   3: in=2.33225 out=5.43938
lane   4: in=2.92146 out=8.53491
lane   5: in=2.89332 out=8.37133
cpu reference written to out/reference.bin
----

TIP: Keep the generated `out/reference.bin` around if you plan to capture GPU buffers; otherwise, delete it to leave the workspace clean.

[[validation-diff]]
== Validating Device Dumps

The diff tool consumes two binary dumps (expected versus captured) and reports mismatched lanes, previewing the first few discrepancies to help you spot data-dependent bugs quickly. It assumes little-endian `f32` values, matching how most host APIs expose raw mapped buffers. link:https://github.com/ziglang/zig/tree/master/lib/std/mem.zig[mem.zig]

[source,zig]
----
include::{sourcedir}/35__project-gpu-compute-in-zig/03_compare_dump.zig[]
----


.Run
[source,shell]
----
$ zig run 03_compare_dump.zig -- out/reference.bin out/reference.bin
----

.Output
[source,shell]
----
mismatched lanes: 0
----

NOTE: To validate a real GPU run, save the device buffer as `out/gpu_result.bin` and rerun `03_compare_dump.zig` against that file to surface any divergence. link:https://github.com/ziglang/zig/tree/master/lib/std/Io.zig[Io.zig]

[[notes-caveats]]
== Notes & Caveats

* Storage buffers require explicit alignment; keep your `extern struct` definitions in lockstep with host descriptor bindings to avoid silent padding bugs.
* The self-hosted SPIR-V backend rejects unsupported address spaces on CPU targets, so isolate kernel source files from host builds (no `@import` from CPU binaries).
* Deterministic PRNG seeding keeps CPU and GPU executions comparable across runs and CI environments.

[[exercises]]
== Exercises

* Extend the kernel to fuse multiplication and addition (`a * a + b`) by binding a second input buffer; update the host and diff tool accordingly.
* Teach the host CLI to emit JSON metadata describing the dispatch plan, so external profilers can ingest the run configuration. link:https://github.com/ziglang/zig/tree/master/lib/std/json.zig[json.zig]
* Capture real GPU output (via Vulkan, WebGPU, or wgpu-native) and feed the binary into `03_compare_dump.zig`, noting any tolerance adjustments required for your hardware.

[[caveats-alternatives-edge-cases]]
== Alternatives & Edge Cases

* Vendors map storage buffers differently; check for required minimum alignments (for example, 16 bytes on some drivers) before assuming `f32` arrays are densely packed.
* For very large buffers, stream comparisons instead of loading entire dumps into memory to avoid allocator pressure on low-end machines.
* When targeting CUDA (`nvptx64`), swap the calling convention to `.kernel` and adjust address spaces (`.global`/`.shared`) to satisfy PTX expectations.
